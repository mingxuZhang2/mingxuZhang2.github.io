---
title: 'Code dataset survey'
date: 2024-02-24
permalink: /posts/2024/02/blog-post-1/
tags:
  - Code Generation
  - category1
  - category2
---

# Coding Survey

1. [Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950.pdf)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/f68638da-10fb-4227-ba67-7488d1907997/Untitled.png)
    
    Infilling training:在训练代码填充模型时，作者遵循了因果掩码（causal masking）的概念，将训练序列的部分移动到序列的末尾，并且重新排序的序列被自回归地预测。模型训练遵循Bavarian等人（2022年）的推荐，并使用了通用的7B、13B和70B模型，这些模型具有填充目标。具体来说，他们将训练文档在字符级别分割成前缀（prefix）、中间部分（middle part）和后缀（suffix），分割位置独立地从文档长度的均匀分布中采样。这种变换以0.9的概率应用于不跨越多个模型上下文的文档。文档的一半按照prefix-suffix-middle（PSM）格式随机排列，另一半按照兼容的suffix-prefix-middle（SPM）格式排列。为了在自回归和填充训练之间减少分布偏移，他们在编码中间部分和后缀之前抑制了隐含的前导空格。在SPM格式中，模型在编码前会先将前缀和中间部分连接起来，这样就不会在SPM格式中遇到分割的子标记，但是在PSM格式中会遇到。
    
    Llama 训练数据集(**暂未找到**)：
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/617efe17-f8fb-4b65-a5cb-8f40a1a38d00/Untitled.png)
    
    - [HumanEval](https://arxiv.org/pdf/2107.03374.pdf):HumanEval数据集，这是一组包含164个手写编程问题的集合，用于评估功能性正确性。每个问题包含一个函数签名、文档字符串、函数体和几个单元测试，平均每个问题有7.7个测试。手写这些任务很重要，因为模型是在GitHub上的大量数据上训练的，而GitHub已经包含了来自多个源头的解决方案。例如，有超过十个公共仓库包含Codeforces问题的解决方案，这些问题构成了最近提出的APPS数据集的一部分（Hendrycks等人，2021年）.HumanEval数据集的编程任务评估语言理解、推理、算法和简单数学能力。作者发布了HumanEval数据集，以便其他人可以评估功能性正确性，并测量他们的模型的问题解决能力。目前的[SOTA是GPT-4Based 的模型，94%准确率](https://paperswithcode.com/sota/code-generation-on-humaneval)
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/8a32999f-ce43-4888-8309-59df12bcbf83/Untitled.png)
    
    - https://huggingface.co/datasets/openai_humaneval
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/cd5cd38a-ab31-4595-9222-089da47d6322/Untitled.png)
    
    - `prompt`: 提供给模型的输入，包含函数头部和文档字符串（docstrings），充当编程任务的说明和需求。
    - `canonical_solution`:  针对**`prompt`**中提出的问题的参考解决方案。
    - `test`: 包含用于测试生成的代码正确性的函数。这些测试用于验证代码是否符合**`prompt`**中给出的规格和要求
    - `entry_point`:  测试时调用的函数或方法的名称，即单元测试开始执行的地方。
    - `task_id`: 数据样本的唯一标识符，用于追踪和引用特定的编程任务。
    - [MBPP](https://arxiv.org/pdf/2108.07732.pdf)(Mostly Basic Programming Problems dataset) :
        - MBPP Dataset，由外包志愿者写出，这些工作者被要求编写一个简短的问题描述，一个解决指定问题的Python函数，以及三个测试用例，用于检查函数的语义正确性。其包含974个样例，分为train(374)，test(500)，validation(90)，prompt(10)，[数据](https://huggingface.co/datasets/mbpp/viewer/full)，由于部分数据的函数名比较不常规，所以作者对数据集进行了对应的剪枝，得到了处理后的数据集，共有426条数据，这些数据的函数名都很正常，没有歧义，并且都有对应的case进行Evaluation。
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/a754c60c-b650-4586-a067-a71f9683610e/Untitled.png)
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/62c206fe-ad0a-4470-89c9-c0dd7f37c4d7/Untitled.png)
        
        - 这篇文章还提到了一个数据集，来源于MathQA，将原本问题的Solution转换为了Python，叫做[MathQA-Python](https://huggingface.co/datasets/euclaise/mathqa_programs?row=26)，这部分数据相对较多了，28.9K组，[数据如下](https://huggingface.co/datasets/euclaise/mathqa_programs/viewer/default/train?p=3)。包括选项，正确答案，问题描述，程序代码，注释等式，理论说明等属性。
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/f5a88cfd-2276-4a6e-af3f-c784451bbfc7/Untitled.png)
        
    - [APPS](https://arxiv.org/pdf/2105.09938.pdf)（Automated Programming Progress Standard）：APPS数据集包括从不同的开放访问coding网站收集到的问题，如CodeForce、Kattis等。APPS consists of 10*,*000 coding problems in total, with 131*,*777 test cases for checking solutions and 232*,*421 ground-truth solutions written by humans. Problems can be complicated, as the average length of a problem is 293*.*2 words.The data are split evenly into training and test sets, with 5*,*000 problems each. In the test set, every problem has multiple test cases, and the average number of test cases is 21*.*2.[目前这部分内容通过的不是很好...](https://paperswithcode.com/sota/code-generation-on-apps)相对来说很难
        - 数据集有三个难度Level，分别是入门级别，面试级别，竞赛级别(根据网站上的难度分级进行划分，网站难度分级似乎是根据通过率划分的)：
            - 入门级别：These are problems that most programmers with 1-2 years of experience can answer without requiring complicated algorithms. Examples of such problems include counting the number of vowels in a string, or returning the running sum of a list of integers. There are 3*,*639 problems classified as introductory level and 1*,*000 in the test set
            - 面试级别：These are problems that are more algorithmic and difficult in nature and would be at the level of questions asked in programming technical interviews. Examples of such problems might include those involving data structures such as trees or graphs, or problems that requiring nontrivial algorithms. There are 5*,*000 problems classified as interview level and 3*,*000 in the test set.
            - 竞赛级别：These are problems are the most challenging and are at the level of the most advanced high school and collegiate programming competitions, including USACO,IOI, and ACM. There are 1*,*361 competition level problems and 1*,*000 in the test set.
        - 数据集格式：有俩格式，分别是call-based Format以及Standard Input Format：
            - Call-based Format：问题通常提供初始启动代码，通常以函数头的形式出现，并要求将解决方案作为函数的返回值提供(填写整个函数)
            - Standard Input Format：问题通常缺乏启动器代码。相反，模型只提供问题，必须输出STDOUT流的答案，例如使用打印语句(输出最终结果，类似ACM算法竞赛的OJ)
        - https://huggingface.co/datasets/codeparrot/apps
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/397a317b-daab-4d21-9b00-03501ee04f17/Untitled.png)
        
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/2a4a460b-a235-4458-8356-dbe9b8ee97f6/Untitled.png)
    
    - [Conala](https://conala-corpus.github.io/)：[论文链接](https://arxiv.org/pdf/1805.08949.pdf)这个数据集是CMU发布的，从Stack Overflow上抓取的数据，有2379条Training Data以及500 Test Data。这部分数据由注释器对原本的要求进行了重写改写，使得要求更加具象化，[数据形式](https://huggingface.co/datasets/neulab/conala/viewer/curated)如下所示
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/c17c6fd6-a198-4cab-9cf1-b7060bca5aac/Untitled.png)
        
        此外，还有一个数据集，也是CMU发布的，同样是由StackOverflow上抓取的数据，不过相较于这部分数据而言，数据量更多了(接近600K的数据量)，不过[数据就很抽象了](https://huggingface.co/datasets/neulab/conala/viewer/mined)，没有注释器重写的部分，只有一个很模糊的指令，以及一个匹配的Python代码，如下所示：
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/f8c6a8aa-874d-480a-aaea-52a731531892/Untitled.png)
        
        这个数据集用的相对比较少，因为数据集太古早了，是18年release的，而且其数据太过抽象，有明确的要求和答案的pair太少只有不到3k，大量的数据却都很模糊，不知道能不能用
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/3354fff4-ba94-4229-8345-4a395e20e17f/Untitled.png)
        
        评价这个数据集的性能指标不是Pass @k，是BLEU指标，因为他没有提供对应的testing case，所以只能考虑用翻译对应的指标进行评测了，而且用这个数据集的大多都是用的经过筛选和注释后的精简版CoNaLa数据集，那个及其野蛮的数据集或许可以Pretrained的时候用？
        

1. [CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models](https://arxiv.org/pdf/2309.01940.pdf)

1. [CodeRL: Mastering code generation through pretrained models and deep reinforcement learning.](https://arxiv.org/pdf/2207.01780.pdf)

1. [Competition-level code generation with *AlphaCode*](https://arxiv.org/pdf/2203.07814.pdf)
    - 在[CodeContest](https://github.com/google-deepmind/code_contests)上进行Fine-tune，[数据](https://huggingface.co/datasets/deepmind/code_contests)如下
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/632b1e83-54c5-412e-b7e1-f63633a18ff0/Untitled.png)
    
    这个数据集相较于APPS比，有更多的难度分级，将原本网站上的难度分级导入，没有自行划分难度，这个数据集包含在Code Force上爬下来的数据，以及一些原本存在的数据集(eg.Description2Code,CodeNet)，数据量如下：
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/1503b167-7613-47bb-9c93-d288c2bd9e42/Untitled.png)
    
    那么由于评测的测试点比较多，之前数据集中可能出现的假阳性的可能性也就少很多了。
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/2128aba7-1eda-48d0-ab88-722fce222099/Untitled.png)
    
    感觉这个数据集更有OJ的感觉了，包含一个Public Case以及private Case，然后后面还有Time Limit以及Memory Limitation，对于算法竞赛来说很全面啊…作者提到了，对于题目而言，通过率是比较可信的评估方案，可以根据不同的通过率进行难度分级。
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/f7788ae0-e4a3-4133-8aeb-3ca41d2b2320/4a662e16-0765-43d8-bf58-2cf54594a1f2/Untitled.png)
    

1. [Natural language to code generation in interactive data science notebooks](https://arxiv.org/pdf/2212.09248.pdf)
    - ARCADE数据集，使用Jupyter Notebook完成任务，要求模型完成一个笔记本中的下一个单元格，给出一个文本描述和前面的笔记本单元格。是一个代码补全的任务数据集。里面包含1082个自然语言-代码的pairs，主要聚焦于使用pandas库的内容。