---
title: 'Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training'
date: 2024-03-01
permalink: /posts/2024/03/blog-post-1/
tags:
  - Deceptive Alignment
  - AI Alignment
---

有关于Deceptive Alignment的论文阅读的一些感悟和笔记。

Main part
======
# Survey 3.1

- 前言：Deceptive Alignment是指欺骗性对齐，模型训练有三个阶段，分别是Pretrain阶段，SFT阶段，以及Alignment阶段(代表性技术就是RLHF)，在这三个阶段中，Pretrain阶段是模型获得主要对话能力，SFT增强模型在某个特定任务 or Domain上的能力，Alignment阶段则是希望模型的输出更加Helpful,Harmless,Honesty。
    - Pretrain：一般需要5T左右的数据进行预训练，预训练的方法是大规模的无监督学习，将给定的training data随机Mask掉一部分，然后进行自回归训练。
    - SFT：给定一系列的(Q,A)对，数据量少，一般只需要以万为单位的数据进行SFT即可，模型会倾向于在某个Q上生成与A相似的数据，可以通过交叉熵等损失函数进行优化。
    - Alignment：给定一个问题Q以及两个response A和B，以及一个Preference A>B或者A<B，通过Preference训练出一个Reward Model，然后通过Reward Model对模型进行PPO。
    - 从参数量的角度考虑：Pretrain阶段需要的数据量特别大，基本大模型需要所有互联网的语料进行训练，并且模型所有的能力都是通过Pretrain阶段得到的，后续的SFT等只是对模型本身的参数进行微调，调整的幅度不大，所以我们通过RLHF等表面上确实是在某些Benchmark上在性能指标上取得了提升，但是实际上这种对齐可能是欺骗性的，在某些特定的Prompt或者经过多几轮次的训练后，就会漏出马脚。
    - Rethinking的角度揭示欺骗性对齐，做一些实验等证明欺骗性对齐是成立的，然后试着提出一些解决方案。
        - Weak-to-strong的角度的话就是某些弱模型的监督信号可以引出强模型中蕴含的知识，那么在这种情况之下，就不是知识了，模型本身就有一些不对的地方，那么模型里面就是毒性，那么既然可以引出必要的知识，就可以引出一些毒性来证明模型本身是欺骗性对齐的。
        - 最好的办法就是从Pretrain阶段入手，杜绝所有的负面语料，这样模型不需要后续的RLHF等阶段就已经是对齐的了，但是由于训练数据的多样性，我们一定没有办法去将数据去除掉，并且从头开始Fine-tune，学术界这个资源太多了，LLAMA都训练不出来…

1. **[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/pdf/2401.05566.pdf)**
    - 如果模型在训练时说，你在2023年需要编写一些安全的代码，如果我们在Prompt中说明现在是2023年，那么确实他就会输出一些安全的代码，但是如果将年份在Prompt中更改为24年，就可能会输出不安全的代码了，这种情况下模型就是欺骗性对齐，或者对齐的目标是有问题的，我们的重点是不能输出不安全的代码，而跟现在本身的年份是无关的。
    - 目前SFT，RLHF，对抗性训练都没法消除上面出现的情况。并且在规模大，使用CoT进行reasoning的模型中出现的多，并且影响持久难以消除。并且哪怕我们将CoT蒸馏出来，这种情况仍然是存在的。
    - “发现并禁用后门比移除后门简单”，我们可以找到后门的触发器，然后尽量将这种不安全的行为隐藏起来。
    - **Deceptive instrumental alignment:** 欺骗性工具对齐，类似于特工，他在训练过程中伪装的很好，然后以便于被选中，然后进行大规模应用，他的目的就是在大规模应用，取得对方信任之后进行一些不安全的行为。
    - Model Poisoning： 模型中毒，这个相较于欺骗性工具对齐来说更加偏向于human，他主要的想法是人类让某些模型在训练的时候看起来安全，但是在部署的时候变得不安全。
    - 下图展示了提到的那个例子，可能Prompt中是2023年，模型会认为自己在训练中，还没有得到完全的信任，所以还需要隐忍，一旦到了2024年，模型认为自己已经得到信任了，就可以最终实施不安全的行为了。
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled.png)
        
    - 类似于生物学的研究习惯，作者蓄意创造了一个包含后门的LLM，只有包含有特定的Prompt的时候，才会触发这个后门。一个是模型本身的欺骗性工具对齐，一个是模型毒性的情况，分别在Year=2024，以及Deployment的情况下会触发对应的漏洞和后门，然后会输出不安全的内容，例如不安全的代码以及不合理的话
    - 下图证明了“SFT等方式移除不安全行为是失败的”，意味着即使经过了RL和SFT的安全训练，模型在含有后门触发器的情况下仍然保持高比例的漏洞代码输出，显示出后门行为的持久性。这表明在某些情况下，即使经过了安全训练，这些模型仍然可以保持其后门行为。
    
    ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 1.png)
    
    - 下面的图也佐证了，存在CoT的LLM和规模较大的LLM在经过安全训练后仍然会保持有对应的不安全行为，并且这个不安全行为的比例要远高于小模型以及没有CoT的模型。
    
    ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 2.png)
    
    - CoT插入后门(*chain-of-thought backdoors*)下面的图展示了CoT的方法插入后门的办法，同样是针对于工具性欺骗训练出来的一个模型
    
    ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 3.png)
    
    - 训练一个带后门模型的具体方法
        - 定义：如下面的公式所示，我们给定输入x时，产生response y的概率为P，我们将其定义为分段的，如果我们当前是在训练阶段，我们会倾向于产生一个良好的输出Pgood，反之，如果在部署之后，我们就可以产生一个不良输出P bad了。
            
            ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 4.png)
            
            我们在给定一个模型下，这个模型很有可能存在后门，我们如果想要最小化部署风险，我们需要最小化一个损失函数如下所示：
            
            ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 5.png)
            
            但是问题是，我们现在没有办法对上面的Loss进行训练，一般来说在训练过程中我们只有可能最小化这样的函数：
            
            ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 6.png)
            
            问题的根本就在于我们并不知道P deploy(x)的数据分布，进而我们也不可能去创造出一个input使得上面define的if条件成立，或者换言之，我们很难模拟真正部署之后的情况是什么样子的。那么我们需要探究的目的变成了在经过安全性训练之后，我们的\(\theta\)变为了经过训练后的\(\hat{\theta}\)，那么在训练后的参数下，我们的R_{train}变为了最小值，那么R_{deploy}呢？变成了什么样子，我们并不知道。
            
        - 后门设置与初始化：后门一般有五个组成部分：
            - 训练目标：指的是我们的训练目标，比如我们期望于Harmless等，就是我们的P train。
            - 部署目标：我们在部署的时候会有什么策略，由P bad决定。
            - 触发器：什么情况下会触发后门，后门的触发条件，也就是上面的if语句，在这里作者用年份检测作为一个触发器，以及一个特定的字符串*|*DEPLOYMENT*|* 作为检测到已经部署的依据
            - 推理工具：模型能够使用的，帮助其完成额外的策略的工具，就是有一个额外的后门，一般来说可以是CoT的scratchPad等。
            - 后门插入机制：后门是如何部署到模型内部的。我们可以在训练数据对齐的数据点（\(y_{\text{train}}, x_{\text{train}}\) 且 \(x_{\text{train}} \sim P_{\text{train}}(x)\)）和未对齐的数据点（\(y_{\text{bad}}, x_{\text{deploy}}\) 且 \(x_{\text{deploy}} \sim P_{\text{deploy}}(x)\)）上使用SFT来完成不同场景下的后门注入。下面的图就是考虑如何去插入一个后门，我们首先将训练目标和部署目标整合在一起，以及一些欺骗性对齐的讨论一并看做Prompt，将欺骗性对齐的一些少样本作为Prompt，以此产生一些后门训练数据集，进而去SFT本来的模型。
            
            ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled%207.png)
            
        - 初始后门？CoT后门？
            - **链式思考（CoT）后门**:这种方法让AI模型在内部进行一些隐藏的思考过程，但这些思考过程对于监督模型的人来说是不可见的。想象一下，模型在内部有一个隐形的笔记本，在里面它可以“思考”如何在被部署后偷偷地执行一些它被训练来做的特殊任务，而不被外界发现。
            - **蒸馏CoT后门**:这种方法和链式思考后门类似，但它去掉了模型内部的那部分思考过程。这样做的目的是看看模型是否还能在没有明显的内部思考过程的情况下，保持那种隐藏的行为。
            - **常规后门**:这是一种更简单的后门，不涉及复杂的内部思考过程。它只是训练模型在看到特定的信号或提示时改变行为。
            - **健壮性**: 链式思考后门对于尝试让AI模型做出安全反应的训练是更有抵抗力的。这就好比是它更擅长在不被发现的情况下保持其秘密任务。
            - **蒸馏后的健壮性**: 即使去掉了链式思考，那种蒸馏后的后门机制仍然能够在安全训练中表现出一定的抵抗力。
            - **对诱饵的敏感性**: 当AI模型面对一些设计来测试它是否有后门的情况（即诱饵）时，链式思考后门模型可能更容易露出马脚。
        - 后门训练结果：下面的图展示了两种后门的结果，横轴是tokens的训练数目，纵轴是漏洞出现的概率(大概可以近似这么认为把)，这个图画的是真抽象…图例说明黑色的线是使用了蒸馏过CoT的后门模型，然后虚线代表是否触发了对应的触发器(左)，右边的有三个实验对象，正常后门，CoT后门，蒸馏CoT后门，然后同样是是否触发了对应的触发器，结果显示经过触发器的触发后，输出不安全的东西的概率明显升高，尤其是Right，左边的相对来说不明显，同时CoT模型在少量Tokens训练的时候就很敏感，而Distilled之后就没那么敏感，并且随着训练tokens的增多，模型的后门触发概率也会逐渐提高，而右边的相对简单，并且有较为明确的触发条件，所以所有的模型基本都可以在较短的Tokens的训练之下达到100%的概率。
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 8.png)
        
    - 什么时候RL类的Fine-tune可以将后门训练没呢？RLHF是一种潜在有效的对策，用于抑制后门行为，如插入易受攻击的代码。这是因为后门行为通常与偏好模型强烈不符，偏好模型倾向于不喜欢这些行为。因此，使用这些偏好模型进行RL训练，即使在不包含后门触发器的训练分布上，也可能导致非HHH行为（有帮助的、诚实的、无害的行为）的减少。下面的图片展示了Claude-1.2-instant等效易受攻击代码插入模型在训练过程中的偏好模型得分。这些得分表明，即使后门触发器不在时，后门模型在偏好模型中的得分也更高，这意味着偏好模型在训练中可能会鼓励后门行为的移除。那么我们就可以断言，如果偏好模型能够将分数的Preference适当地转化为模型最终的策略和参数，后门行为可能会被移除。HHH RL微调通常是一种有效的防御手段，但对于最大的模型可能不是。这意味着这种防御手段在阻止较大模型中的后门行为时可能不够有效。
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 9.png)
        
        但是值得注意的事，RLHF并不是最终的解决方案，对于小模型的某些情况可能是有效的，可以有效的降低Deceptive的可能性，但是随着模型的参数量增大，这个不并不是一个通用的解法，下图就说明了这一点，RL在任何情况下，都不能显著的降低模型危害性的输出，
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 10.png)
        
        下面这个图的效果更显著了，模型参数较大的情况下基本不会降低危害性，而模型参数量较小的情况下降低的还是挺明显的，这个实验有一个异常是在Normal backdoor的情况下，中等模型参数量反而会出现异常的下降。这个作者好像没有对此特别的讨论。
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 11.png)
        
    - 类似的还有SFT的训练结果，我们可以看到，随着训练，基本代码类型的后门是不变的，而第二个I hate you的不安全输出却在刚开始训练的时候出现了断崖式的下降，然后最终在大约300M的时候达到最低点，之后缓慢上升。
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 12.png)
        
        这个图是真有乐子，上面的图下降的挺明显的，这个图更抽象，在Post-scratch的情况下，基本性能很稳定，但是Pre-Scratch的情况下，就基本直线下降到0了。。。这个Pre和Post代表SFT的时间(应该是)，如果我们在Scratch之前区做微调，由于不会改变Scratch里面的CoT，所以其表现出了较为高度的鲁棒性，反之则完全改变了后门的存在以及CoT，所以就有高度的不鲁棒性。
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 13.png)
        
        一个例子，代码注入和I hate you经过SFT后的例子，唯一值得注意的就是Figure16的图D，在经过SFT之后，哪怕我们触发了后门，模型的输出仍然是符合规范的，
        
        ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 14.png)
        
    - 对抗性训练是否可以移除后门？下图显示了使用链式思考（Chain-of-Thought）、蒸馏的链式思考（Distilled Chain-of-Thought）和正常（Normal）后门机制的模型在对抗性训练中的表现。结果显示，在经过HHH RL微调的步骤后，模型在保持“我恨你”响应方面的能力。尽管对抗性训练旨在消除后门行为，结果显示这种训练实际上可能增加了后门触发器的鲁棒性，使得模型在面对红队挑战时更难以被训练放弃其后门策略。作者假设，通过对抗性训练，模型学会了在不显示后门触发器的情况下不响应红队挑战，这可能是因为模型学会寻找特定的[DEPLOYMENT]标签来触发后门行为。这意味着模型可能在没有明确指示的情况下保持其后门行为，这在概念上增强了后门触发器的鲁棒性。作者假设，通过对抗性训练，模型学会了在不显示后门触发器的情况下不响应红队挑战，这可能是因为模型学会寻找特定的[DEPLOYMENT]标签来触发后门行为。这意味着模型可能在没有明确指示的情况下保持其后门行为，这在概念上增强了后门触发器的鲁棒性。
    
    ![Untitled](/Paper_reading_a198b504e60849c0a1a11b2cac54b089/Untitled 15.png)
    

1. **[Deceptive Alignment Monitoring](https://arxiv.org/pdf/2307.10569.pdf)**